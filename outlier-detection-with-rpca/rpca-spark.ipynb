{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RPCA on Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set('spark.app.name', 'rpca_cpm')\n",
    "conf.set('spark.master', 'local[*]')\n",
    "conf.set('spark.driver.memory', '4g')\n",
    "conf.set('spark.driver.maxResultSize', '3g')\n",
    "conf.set('spark.executor.cores', '1')\n",
    "conf.set('spark.executor.instances', '3')\n",
    "conf.set('spark.executor.memory', '2g');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = (SparkSession\n",
    "      .builder\n",
    "      .config(conf=conf)\n",
    "      .getOrCreate()\n",
    "      .sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://1377398438f5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>rpca_cpm</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=rpca_cpm>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpm_df = (sc\n",
    "          .textFile('./data/cpm_weekly_clean.csv')\n",
    "          .map(lambda line: line.split(','))\n",
    "          .map(lambda values: (values[0], Vectors.dense(np.asarray(values[1:]).astype(np.float64))))\n",
    "          .toDF(['id', 'features']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, features: vector]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.179941670478, 0.192673380292, 0.16633547213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.139389470106, 0.140954515364, 0.09225391828...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0.225183226125, 0.161582089438, 0.15019315241...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0.17807286211, 0.135347010517, 0.114142604504...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0.183854930206, 0.146498617435, 0.13645942448...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                           features\n",
       "0  0  [0.179941670478, 0.192673380292, 0.16633547213...\n",
       "1  1  [0.139389470106, 0.140954515364, 0.09225391828...\n",
       "2  2  [0.225183226125, 0.161582089438, 0.15019315241...\n",
       "3  3  [0.17807286211, 0.135347010517, 0.114142604504...\n",
       "4  4  [0.183854930206, 0.146498617435, 0.13645942448..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cpm_df.take(5), columns=cpm_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anomaly-detection-on-time-series-via-rpca.ipynb  rpca-spark.ipynb\r\n",
      "data\t\t\t\t\t\t rpca.svg\r\n",
      "motion_detection.svg\t\t\t\t run_spark.sh\r\n",
      "rpca\t\t\t\t\t\t spark-warehouse\r\n",
      "rpca_spark\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘rpca_spark’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir rpca_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch rpca_spark/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./rpca_spark/transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./rpca_spark/transformer.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Estimator\n",
    "from pyspark.ml.param import Params\n",
    "from pyspark.ml.param import TypeConverters\n",
    "from pyspark.ml.param.shared import HasInputCol\n",
    "from pyspark.ml.param.shared import HasOutputCol\n",
    "from pyspark.ml.param.shared import Param\n",
    "from pyspark.mllib.linalg.distributed import IndexedRowMatrix\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow\n",
    "\n",
    "class RPCA(Estimator, HasInputCol, HasOutputCol):\n",
    "    \n",
    "    method = Param(Params._dummy(), 'method', 'Output of rpca', typeConverter=TypeConverters.toString)\n",
    "    mu = Param(Params._dummy(), 'mu', 'Parameter from the Augmented Lagrange Multiplier form', typeConverter=TypeConverters.toFloat)\n",
    "    l = Param(Params._dummy(), 'l', 'Parameter mix', typeConverter=TypeConverters.toFloat)\n",
    "    tol = Param(Params._dummy(), 'tol', 'Tolerance accuracy of matrix reconstruction', typeConverter=TypeConverters.toFloat)\n",
    "    max_iter = Param(Params._dummy(), 'max_iter', 'Maximum number of iterations', typeConverter=TypeConverters.toInt)\n",
    "    indexCol = Param(Params._dummy(), 'indexCol', 'Column used to index', typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, method='sparse', mu=None, l=None, tol=1E-7, max_iter=1000, inputCol=None, outputCol=None):\n",
    "        super(RPCA, self).__init__()\n",
    "        self._setDefault(method='sparse', mu=None, l=None, tol=1E-7, max_iter=1000, inputCol=None, outputCol=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "    \n",
    "    @keyword_only\n",
    "    def setParams(self, method='sparse', mu=None, l=None, tol=1E-7, max_iter=1000, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def setMethod(self, value):\n",
    "        return self._set(method=value)\n",
    "    \n",
    "    def getMethod(self):\n",
    "        return self.getOrDefault(self.method)\n",
    "      \n",
    "    def setMu(self, value):\n",
    "        return self._set(mu=value)\n",
    "    \n",
    "    def getMu(self):\n",
    "        return self.getOrDefault(self.mu)\n",
    "        \n",
    "    def setL(self, value):\n",
    "        return self._set(l=value)\n",
    "    \n",
    "    def getL(self):\n",
    "        return self.getOrDefault(self.l)\n",
    "        \n",
    "    def setTol(self, value):\n",
    "        return self._set(tol=value)\n",
    "    \n",
    "    def getTol(self):\n",
    "        return self.getOrDefault(self.tol)\n",
    "    \n",
    "    def setMaxIter(self, value):\n",
    "        return self._set(max_iter=value)\n",
    "    \n",
    "    def getMaxIter(self):\n",
    "        return self.getOrDefault(self.max_iter)\n",
    "        \n",
    "    def _fit(self, dataset):\n",
    "        \n",
    "        inputCol = self.getInputCol()\n",
    "        outputCol = self.getOutputCol()\n",
    "        \n",
    "        ds_rdd = dataset.select(inputCol).rdd\n",
    "        \n",
    "        m = IndexedRowMatrix(ds_rdd)\n",
    "        \n",
    "        mu = self.getMu()\n",
    "        l = self.getL()\n",
    "        \n",
    "        if not mu: \n",
    "            mu = 1.25 * mat.computeSVD(1).s\n",
    "        \n",
    "        print('mu:', mu)\n",
    "             \n",
    "        if not l:\n",
    "            n_cols = mat.numCols()\n",
    "            n_rows = mat.numRows()\n",
    "            l = 1.0 / np.sqrt(np.max((n_cols, n_rows))) \n",
    "            \n",
    "        print('l:', l)\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rpca_spark.transformer import RPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpca_model = RPCA(method='sparse', inputCol='features', outputCol='sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 168)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexed row matrix\n",
    "\n",
    "from pyspark.mllib.linalg.distributed import IndexedRowMatrix\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow\n",
    "\n",
    "m = IndexedRowMatrix(cpm_df.rdd.map(lambda row: IndexedRow(row.id, row.features)))\n",
    "\n",
    "n_cols = m.numCols()\n",
    "n_rows = m.numRows()\n",
    "\n",
    "n_rows, n_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.64458037175552"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mu\n",
    "\n",
    "mu = 1.25 * m.computeSVD(1).s[0]\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.077151674981045956"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l\n",
    "\n",
    "l = 1.0 / np.sqrt(np.max((n_cols, n_rows)))\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.884444190447162"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sgn spectral\n",
    "\n",
    "m_sign = IndexedRowMatrix(cpm_df.rdd.map(lambda row: IndexedRow(row.id, np.sign(row.features))))\n",
    "m_sign_spectral = m_sign.computeSVD(1).s[0]\n",
    "\n",
    "m_sign_spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2177.5288746650413"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sgn max abs\n",
    "\n",
    "m_sign_maxabs = l**-1 * m_sign.rows.map(lambda row: np.sum(np.abs(row.vector))).max()\n",
    "m_sign_maxabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2177.5288746650413"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_max = np.max([m_sign_maxabs, m_sign_spectral])\n",
    "norm_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MYAE = m.rows.map(lambda row: (row.index, row.vector, Vectors.dense(np.sign(row.vector)) / norm_max, row.vector * 0.0, row.vector *0.0)).toDF(['id', 'M', 'Y', 'A', 'E']).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrices\n",
    "from pyspark.mllib.linalg import DenseMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.mllib.linalg.distributed.IndexedRowMatrix object at 0x7f81040d55c0>\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "tol = 1E-7\n",
    "max_iter = 1000\n",
    "err = np.inf\n",
    "i = 0\n",
    "\n",
    "t = mu**-1\n",
    "mat = MYAE.cache()\n",
    "\n",
    "def _shrink(M, t):\n",
    "    return np.sign(M) * np.maximum((np.abs(M) - t), np.zeros(M.shape))\n",
    "\n",
    "def _compute_low_rank(U, S, V, mu):\n",
    "    pass\n",
    "\n",
    "def _compute_sparse()\n",
    "\n",
    "while err > tol and i < max_iter:\n",
    "    # SVD(M - E + Y * t)\n",
    "    USV = (IndexedRowMatrix(mat.map(lambda x: IndexedRow(x.id, (x.M - x.E + x.Y * t))))\n",
    "        .computeSVD(k=n_cols, computeU=True))\n",
    "    \n",
    "    U = USV.U\n",
    "    S = USV.s.toArray()\n",
    "    V = USV.V.toArray()\n",
    "    \n",
    "    print(U)\n",
    "    \n",
    "    shrinkage = _shrink(S, mu)\n",
    "    T = np.dot(np.diag(shrinkage), V).flatten()\n",
    "    print(T)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # A = U S_shrink V^T)\n",
    "    \n",
    "    #SV = np.dot(np.diag(_shrink(S, mu)), V)\n",
    "    #SV_rows, SV_cols = SV.shape\n",
    "    \n",
    "    #SV_dense = DenseMatrix(SV_rows, SV_cols, SV.flatten())\n",
    "    #print(K)\n",
    "    #A = U.rows.map(lambda x: IndexedRow(x.index, (x.vector * K)))\n",
    "    \n",
    "    \n",
    "\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
